{"cells":[{"cell_type":"markdown","source":["# Assignment 2 APACHE-SPARK\n## PART A\n#### 1. Count the odd and even numbers using the file ‘integer.txt’"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d4a650af-fa9c-457e-bdcf-c26d0925e141","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"df0faef6-a541-4f7d-9b3b-ad33fa80f4fd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName(\"1628 A2\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"beda0e38-1a1b-4202-bf99-5db1befe4667","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["integerRDD = spark.sparkContext.textFile(\"/FileStore/tables/integer.txt\")\nintegerRDD = integerRDD.flatMap(lambda line: line.split())\neven = integerRDD.filter(lambda x: int(x) % 2 == 0)\nodd  = integerRDD.filter(lambda x: int(x) % 2 == 1)\n\nprint('There are {} even numbers.'.format(len(even.collect())))\nprint('There are {} even numbers.'.format(len(odd.collect())))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bfdd5416-d6cf-475d-aee6-0eb511526f83","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["There are 514 even numbers.\nThere are 496 even numbers.\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 2. Calculate the salary sum per department using the file ‘salary.txt’\n  - Show the department name and salary sum."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4a4b0e9c-5a0b-45fe-bee5-d48de161f6b1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["salaryRDD = spark.sparkContext.textFile(\"/FileStore/tables/salary.txt\")\narrayRDD = salaryRDD.map(lambda x: x.split(\" \"))\nkvRDD = arrayRDD.map(lambda x: (x[0], int(x[1])))\nsumRDD = kvRDD.reduceByKey(lambda x,y: x+y)\nsumRDD.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b38616f5-9806-4301-a322-039b05da6afa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[4]: [('Sales', 3488491),\n ('Research', 3328284),\n ('Developer', 3221394),\n ('QA', 3360624),\n ('Marketing', 3158450)]"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 3. Implement MapReduce using Pyspark on file ‘shakespeare.txt’. \n   - Show how many times these particular words appear in the document: Shakespeare, why, Lord, Library, GUTENBERG, WILLIAM, COLLEGE and WORLD. \n   - (Count exact words only (marks will be deducted for incorrect lowercase/uppercase))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ea15a5dd-22fb-43d5-8dc0-55eaa59e278f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["shakespeareRDD = spark.sparkContext.textFile(\"/FileStore/tables/shakespeare_1.txt\")\nword_ls = ['Shakespeare', 'why', 'Lord', 'Library', 'GUTENBERG', 'WILLIAM', 'COLLEGE','WORLD']\ntemp = shakespeareRDD.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\nsumRDD = temp.reduceByKey(lambda x,y: x+y)\nwordRDD = sumRDD.filter(lambda x: x[0] in word_ls)\nwordRDD.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ed6b7422-cb88-4439-ab9b-1f1140605680","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[5]: [('Shakespeare', 22),\n ('GUTENBERG', 99),\n ('WILLIAM', 115),\n ('WORLD', 98),\n ('COLLEGE', 98),\n ('why', 91),\n ('Lord', 341),\n ('Library', 2)]"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 4. Calculate the top 15 and bottom 15 words using the file ‘shakespeare.txt’.\n  - Show 15 words with the most count and 15 words with the least count. \n  - You can limit by 15 in ascending and descending order of count."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"de36dfad-5b4c-43a1-8c86-ff9eabaea8dc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Sort the RDD by word frequency in ascending order\nmostFreq_15  = sumRDD.sortBy(lambda x: x[1], ascending=False).take(15)\nleastFreq_15  = sumRDD.sortBy(lambda x: x[1], ascending=True).take(15)\nprint(\"The 15 most frequent words with each occurrence number:\\n \", mostFreq_15)\nprint('--------------------------------------------------------------')\nprint(\"The 15 least frequent words with each occurrence number:\\n \", leastFreq_15)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f6861a37-8fd6-4965-bf07-5ff8dd1641f5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["The 15 most frequent words with each occurrence number:\n  [('', 231583), ('the', 11397), ('and', 8777), ('I', 8556), ('of', 7873), ('to', 7421), ('a', 5672), ('my', 4913), ('in', 4600), ('you', 4060), ('And', 3547), ('that', 3522), ('is', 3481), ('his', 3226), ('with', 3175)]\n--------------------------------------------------------------\nThe 15 least frequent words with each occurrence number:\n  [('anyone', 1), ('restrictions', 1), ('whatsoever.', 1), ('re-use', 1), ('online', 1), ('www.gutenberg.org', 1), ('COPYRIGHTED', 1), ('eBook,', 1), ('Details', 1), ('guidelines', 1), ('file.', 1), ('Author:', 1), ('Posting', 1), ('1,', 1), ('2011', 1)]\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###### Note: there were many other words which appeared once only as well."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22abab56-366b-4b64-8337-de5b91c06135","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# PART B\n- work with a distributed recommender system.\n- To do this, create a recommender system using Apache Spark. Things that were taken into consideration were the efficiency of the systems as well as Spark’s complexity."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"94da2cb7-c52d-4a24-a10d-e85ddfa60d44","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 1. Describe your data. \n  - Calculate the top 20 movies with the highest ratings and the top 15 users who provided the highest ratings."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e5b0bd49-bc00-4ee6-9799-621647a1f45b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# File location and type\npath = \"/FileStore/tables/movies.csv\"\n\ndf = spark.read \\\n  .format(\"csv\") \\\n  .option(\"inferSchema\", True) \\\n  .option(\"header\", True) \\\n  .option(\"sep\", ',') \\\n  .option(\"path\", path) \\\n  .load()\n#   .option('nanValue', ' ')\\\n#   .option('nullValue', ' ')\\\n\ndf.show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7c27ae42-dd1d-4065-9ff3-0fb2219b2ae5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+------+\n|movieId|rating|userId|\n+-------+------+------+\n|      2|     3|     0|\n|      3|     1|     0|\n|      5|     2|     0|\n+-------+------+------+\nonly showing top 3 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###### Get basic info, get to know the data first"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"35017972-81a8-4ddf-ab38-19f548a86b16","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\n# Calculate the number of unique ratings, movies, and users\nnum_ratings = df.select(\"rating\").distinct().count()\nnum_movies = df.select(\"movieId\").distinct().count()\nnum_users = df.select(\"userId\").distinct().count()\n\n# rating options & thighest rating\nrating_options = df.select(\"rating\").distinct().rdd.flatMap(lambda x: x)\nhighest_rating = rating_options.max()\n\n# Calculate the number of ratings with the highest mark\nnum_highest_ratings = df.filter(df.rating == highest_rating).count()\n\nprint(\"Num of ratings (choose from:{}): {}\\nNum of movies: {}\\nNum of users: {}\".format(\n    sorted(rating_options.collect()), num_ratings, num_movies, num_users))\nprint(\"Note: There are {} ratings with the highest mark {}.\".format(num_highest_ratings, highest_rating))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2af6b8cb-72dc-4367-9a70-f51403e972f7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Num of ratings (choose from:[1, 2, 3, 4, 5]): 5\nNum of movies: 100\nNum of users: 30\nNote: There are 75 ratings with the highest mark 5.\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###### top 20 movies with the highest ratings, metric chosen: average scores of each movie"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"36ab7b17-e5f5-4b20-a69e-031e898c6e20","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Calculate the average ratings for each movie\nave_rating = df.groupBy(\"movieId\").avg(\"rating\")\ntemp = ave_rating.orderBy(desc(\"avg(rating)\")).limit(20)\ntop_20_movies = temp.withColumn(\"avg(rating)\", round(col(\"avg(rating)\"), 2))   #round values\ntop_20_movies.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bfa39565-3c34-43a1-9b42-99a8e38353bc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-----------+\n|movieId|avg(rating)|\n+-------+-----------+\n|     32|       2.92|\n|     90|       2.81|\n|     30|        2.5|\n|     94|       2.47|\n|     23|       2.47|\n|     49|       2.44|\n|     29|        2.4|\n|     18|        2.4|\n|     52|       2.36|\n|     53|       2.25|\n|     62|       2.25|\n|     92|       2.21|\n|     46|        2.2|\n|     68|       2.16|\n|     87|       2.13|\n|      2|       2.11|\n|     69|       2.08|\n|     27|       2.07|\n|     88|       2.06|\n|     22|       2.05|\n+-------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###### 15 users who provided the highest ratings, metric chosen: average scores of each user"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"906d0952-6b2a-4f99-b90d-55e550aef529","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["ave_rating = df.groupBy(\"userId\").avg(\"rating\")\ntemp = ave_rating.orderBy(desc(\"avg(rating)\")).limit(15)\ntop_15_users = temp.withColumn(\"avg(rating)\", round(col(\"avg(rating)\"), 2))\ntop_15_users.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"51bddf82-49cb-4893-9360-09803fce32e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+-----------+\n|userId|avg(rating)|\n+------+-----------+\n|    11|       2.29|\n|    26|        2.2|\n|    22|       2.16|\n|    23|       2.13|\n|     2|       2.07|\n|    17|       1.96|\n|     8|        1.9|\n|    24|       1.88|\n|    12|       1.85|\n|     3|       1.83|\n|    29|       1.83|\n|    28|       1.82|\n|     9|       1.79|\n|    14|       1.79|\n|    16|       1.78|\n+------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 2. Split the dataset into train and test. \n   - Try 2 different combinations: 70/30,and 80/20.\n\n#### 3. Explain MSE, RMSE and MAE. \n   - Compare and evaluate both of your models with evaluation metrics (RMSE or MAE)\n   - Describe which one works better and why?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"18d32ce2-476f-4568-a943-e0574e7fbca3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###### 80 & 20"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46bd91ee-44b4-4990-b20b-6abc99829151","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n\n################### Create train and test dataset, here we use 0.8 & 0.2 #################\n\n(train_82, test_82) = df.randomSplit([0.8, 0.2],seed=1628)\n\n# Build the recommendation model using ALS on the training data\nals = ALS(userCol= 'userId', itemCol= 'movieId', ratingCol= 'rating', coldStartStrategy = 'drop')\nmodel_82 = als.fit(train_82)\npred_82 = model_82.transform(test_82)\n\n# RMSE on the test data\nevaRMSE_82 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\nrmse_82 = evaRMSE_82.evaluate(pred_82)\nprint(\"RMSE of 80% train and 20% test: \", rmse_82)\n\n# MAE on the test data\nevaMAE_82 = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\nmae_82 = evaMAE_82.evaluate(pred_82)\nprint(\"MAE of 80% train and 20% test: \", mae_82)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"95047974-bc43-42bb-944f-2188c59d37f3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["RMSE of 80% train and 20% test:  1.057657381718659\nMAE of 80% train and 20% test:  0.7654315561480369\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###### 70 & 30"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5689e98f-0db8-4535-8b0a-daa1a83a0fe2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["################### Create train and test dataset, here we use 0.7 & 0.3 #################\n(train_73, test_73) = df.randomSplit([0.7, 0.3], seed=1628)\nmodel_73 = als.fit(train_73)\npred_73 = model_73.transform(test_73)\n\n# RMSE on the test data\nevaRMSE_73 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\nrmse_73 = evaRMSE_73.evaluate(pred_73)\nprint(\"RMSE of 70% train and 30% test: \", rmse_73)\n\n# MAE on the test data\nevaMAE_73 = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\nmae_73 = evaMAE_73.evaluate(pred_73)\nprint(\"MAE of 70% train and 30% test: \", mae_73)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b489adb2-4873-40ff-9fe1-79bd4a9f581f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["RMSE of 70% train and 30% test:  1.0873905618996584\nMAE of 70% train and 30% test:  0.7619639403651358\n"]}],"execution_count":0},{"cell_type":"markdown","source":["- Description of the metrics\n   - Mean Squared Error (MSE): Determined by averaging the squared differences between the predicted values and the actual values. By squaring the errors, MSE assigns higher significance to larger deviations. MSE magnifies larger errors, making them contribute more to the overall loss, which makes the MSE particularly useful for regression problems where significant deviations from the true values need to be penalized more.\n   - Root Mean Squared Error (RMSE): Square root of MSE. \n   - Mean Absolute Error (MAE): Calculated by averaging the absolute differences between the predicted values and the actual values. Unlike MSE with squaring the differences, MAE is less influenced by outliers, which is useful when the outliers or large errors should not be overly penalized. \n\n- Result\n   - RMSE of 70% and 30%:  1.08739 |\n     MAE of 70% and 30%:  0.76196 \n\n   - RMSE of 80% and 20%:  1.05766 |\n      MAE of 80% and 20%:  0.76543\n\n- I may choose RMSE as the metric as the difference is bigger, possibly indicating a clear comparison with less impact of randomness. And for regression problems, significant deviations from the true values need to be penalized more.\n - The difference between the MAE values is minimal, suggesting that both models perform similarly in terms of absolute errors. Based on the evaluation metrics, the model with the 80/20 split appears to work better overall. It achieves a lower RMSE, indicating better accuracy, while maintaining a competitive MAE value. The larger training set (80%) may have provided more data for the model to learn from, resulting in improved performance on the test data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"38d910a6-1823-4c83-8a4a-2001f379b769","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 4. Now tune the parameters of your algorithm to get the best set of parameters. \n- Explain different parameters of the algorithm which you have used for tuning your algorithm. \n\n  - rank: Rank determines how many hidden features the ALS model here will learn. These features are like hidden qualities which can help the model understand and make predictions about users and items(movies here). Defaults to 10.\n\n  - maxIter: It specifies the maximum number of times the ALS algorithm here will repeat its calculations to find the best model. More iterations can improve accuracy, but it takes more time. Defaults to 10.\n\n  - regParam: Control the regularization strength in ALS here. Regularization prevents the model from overfitting by adding a penalty for fitting the data too closely. The regParam value adjusts the intensity of this penalty. Defaults to 1.0 \n  \n- Evaluate all your models again. Show your code with the best values and output."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"92ca7540-7507-4dbf-bb36-9ed2c1b5b0d4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###### 80 & 20"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22830393-ecaf-4e3c-b69d-ff8c54f49290","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Based on code from https://spark.apache.org/docs/latest/ml-tuning.html#train-%20validation-split and course material.\n\n#Tune model using ParamGridBuilder\nparameters= ParamGridBuilder().addGrid(als.rank,[10,15,20]).addGrid(als.maxIter,[10,15,20]).addGrid(als.regParam,[0.01,0.1,1]).build()\n\n#Use model evaluator as RMSE\neval = RegressionEvaluator(metricName= 'rmse',labelCol= 'rating', predictionCol= 'prediction')\n\n#Build train validation split, 80% of the data will be used for training, 20% for validation.\ntrainvs = TrainValidationSplit(estimator=als, estimatorParamMaps=parameters, evaluator=eval,trainRatio=0.8)\n\n#Build cross validator, if needed\n#cv = CrossValidator(estimator=als, estimatorParamMaps=parameters,evaluator=eval, numFolds=3)\n\n#Run TrainValidationSplit, and choose the best set of parameters\n\n################################### For 80 & 20 spliiting ###################################################\nmodel_82 = trainvs.fit(train_82)  \nbest_model_82 = model_82.bestModel  \npred_82 = best_model_82.transform(test_82)   \nrmse_82 = eval.evaluate(pred_82)   \n\nprint(\"Best Paramters combinations for 80&20 splitting: \")\nprint(\"\\nrank: \" , best_model_82._java_obj.parent().getRank())\nprint(\"maxIter: \" , best_model_82._java_obj.parent().getMaxIter())\nprint(\"regParam: \" , best_model_82._java_obj.parent().getRegParam())\nprint(\"RMSE of test set: \", rmse_82) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b9c046a5-740c-4398-9620-b1fafe8e8ca0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Best Paramters combinations for 80&20 splitting: \n\nrank:  15\nmaxIter:  20\nregParam:  0.1\nRMSE of test set:  1.0350194267005735\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###### 70 & 30"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60281af8-6506-4fd5-b165-a5f2961fd5fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#################################### For 70 & 30 Spliiting ################################################\n\nmodel_73 = trainvs.fit(train_73)\nbest_model_73 = model_73.bestModel  \npred_73 = best_model_73.transform(test_73)\nrmse_73 = eval.evaluate(pred_73)\n\nprint(\"Best Paramters combinations for 70&30 splitting: \")\nprint(\"\\nrank: \" , best_model_73._java_obj.parent().getRank())\nprint(\"maxIter: \" , best_model_73._java_obj.parent().getMaxIter())\nprint(\"regParam: \" , best_model_73._java_obj.parent().getRegParam())\nprint(\"RMSE of test set: \", rmse_73) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"18eae4d5-4f5c-4b2a-9289-c7cc741236ea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Best Paramters combinations for 70&30 splitting: \n\nrank:  15\nmaxIter:  15\nregParam:  0.1\nRMSE of test set:  1.0541131040702143\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Although the best values may occured at the end of the range, I just performed how to do the tunning. Yes we can definately increase the possible value for hyperparameters for a better result! This would take much more time to be run, not just for model tunning, but also for further use. For exmaple, the maxIter specifies the maximum number of times the ALS algorithm will repeat its calculations to find the best model, which would lead to huge time expense.\n- The bese value of the hyparameters with model (80 training and 20 testing):\n  - rank:  15\n  - maxIter:  20\n  - regParam:  0.1\n- RMSE of test set of this model:  1.0350"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f02ff1a1-047e-44bc-8992-9ed7bba027dd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 5. Calculate the top 15 movie recommendations for user id 10 and user id 14. Show your code and output.\n- Choose best model with 80 training and 20 testing data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c16aa42f-e45d-4760-b9b5-088baf7cadd4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["userRecs = best_model_82.recommendForAllUsers(15)\n\nprint(\"For user id 10\")\nuser10 = userRecs.filter(F.col('userId') == 10)\nuser10.show(truncate = False)\n\nprint(\"For user id 14\")\nuser14 = userRecs.filter(F.col('userId') == 14)\nuser14.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0a421f3e-1863-405b-8bfb-12244d83aa3f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["For user id 10\n+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|userId|recommendations                                                                                                                                                                                                                                           |\n+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|10    |[{2, 3.3830044}, {40, 3.1212664}, {92, 2.8936815}, {49, 2.7336671}, {25, 2.6379092}, {12, 2.525733}, {62, 2.4848144}, {42, 2.4519153}, {81, 2.4222684}, {4, 2.2977078}, {0, 2.1965504}, {82, 2.1381712}, {95, 2.0598233}, {91, 2.0398493}, {9, 1.9593117}]|\n+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nFor user id 14\n+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|userId|recommendations                                                                                                                                                                                                                                        |\n+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|14    |[{29, 4.4898868}, {52, 4.447929}, {76, 4.160809}, {63, 4.153109}, {62, 3.6495514}, {96, 3.5126195}, {72, 3.4582915}, {58, 3.2557003}, {53, 3.1596372}, {2, 3.0817947}, {85, 2.8149042}, {93, 2.69964}, {47, 2.54888}, {60, 2.5385776}, {67, 2.5166438}]|\n+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##### TOP 15 movies: \n- for user ID 10: Movie  2, 40, 92, 49, 25, 12, 62, 42, 81, 4, 0, 82, 95, 91, 9\n- for user ID 14: Movie 29, 52, 76, 63, 62, 96, 72, 58, 53, 2, 85, 93, 47, 60, 67"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a12c27d-ac9a-46fd-8a76-011be1a3e2ab","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1628 A2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
